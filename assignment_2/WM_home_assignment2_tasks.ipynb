{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webmining - Assignment 2\n",
    "\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. It familiarizes you with basics of Markov Chain and link prediction on graphs.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 2-3 people until 16.06.2020 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Double check if your code relies on presence of files or directories other than those mentioned\n",
    "    in given tasks. Tests run under Linux, hence don't use Windows style paths \n",
    "    (`some\\path`, `C:\\another\\path`). Also, use paths only that are relative to and within your\n",
    "    working directory (OK: `some/path`, `./some/path`; NOT OK: `/home/alice/python`, \n",
    "    `../../python`).\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for only the mean, prints an output instead of returning it, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members (you may add or remove items from the dictionary)\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Pavel',\n",
    "        'last_name': 'Raschetnov',\n",
    "        'student_id': 404839\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Anya',\n",
    "        'last_name': 'Poudyal',\n",
    "        'student_id': 391805\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Philipp',\n",
    "        'last_name': 'Stein',\n",
    "        'student_id': 397615\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Markov chains (6 points total)\n",
    "\n",
    "For this task you are only allowed to use numpy and pandas. For this task always include the RESET states.\n",
    "\n",
    "### Information: data structures for this assignment:\n",
    "A *state* or *item* is always given by a string.\n",
    "A uni-variate sequence is given (or should be implemented as) a list of states (strings).\n",
    "\n",
    "Example:\n",
    "\n",
    "```uni_variate_sequence = [\"a\", \"c\", \"b\", \"a\"]```\n",
    "\n",
    "```list_of_sequences =  [[\"a\"], [\"a\", \"b\", \"c\"], [\"a\", \"c\"], [\"c\"]]```\n",
    "\n",
    "We usually refer to a list of sequences as just `sequences`\n",
    "\n",
    "## 1a) Transitions generator (1)\n",
    "Write a function `generate_transitions(sequences, order, include_RESET=True)` that loops over each of the sequences in the provided list of sequences and `yield`s one transition after another. A transition is a tuple where the first element is a tuple containing the relevant history (i.e. one element for order 1, two elements for order two ...). The second element is the next state. Whether reset states should be included can be specified through `include_RESET`. Use `\"RESET\"` as the indicating string of the reset state (you can assume this string does not occur in the data otherwise). If a sequence is shorter than order + 1 and include_RESET = False, omit that sequence.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "test_sequences = [[\"a\", \"b\"]]\n",
    "list(generate_transitions(test_sequences, order))\n",
    "# order=1 [(('RESET',), 'a'), (('a',), 'b'), (('b',), 'RESET')]\n",
    "# order=2 [(('RESET', 'RESET'), 'a'), (('RESET', 'a'), 'b'), (('a', 'b'), 'RESET')]\n",
    "test_sequences = [[\"a\"], [\"b\"]]\n",
    "list(generate_transitions(test_sequences, 1))\n",
    "# order=1 [(('RESET',), 'a'), (('a',), 'RESET'), (('RESET',), 'b'), (('b',), 'RESET')]\n",
    "```\n",
    "\n",
    "\n",
    "## 1b) Fitting the model (1)\n",
    "Implement the function `fit_mc(sequences, order)` that fits a Markov chain of a specific `order` to the given input of a list of sequences.\n",
    "The function should return t_m, row_names, col_names with\n",
    "\n",
    "* `t_m`: transition_probability matrix (2-d numpy array of type float)\n",
    "* `row_names`: List of source state tuples in the ordering used in the array. \n",
    "* `col_names`: List of the target states in the ordering used in the array\n",
    "\n",
    "Do not include empty rows i.e. rows that have not been observed.\n",
    "\n",
    "\n",
    "Example:\n",
    "```python\n",
    "t_m = np.array([[0.0, 0.5, 0.5],\n",
    "                [0.2, 0.8, 0.0],\n",
    "                [0.3, 0.4, 0.4]])\n",
    "row_names = [\"RESET\", \"state_a\", \"state_b\"]\n",
    "col_names = [(\"RESET\",), (\"state_a\",), (\"state_b\",)]\n",
    "```\n",
    "means that the transition probability to go from \"state_a\" to \"RESET\" is 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transitions(sequences, order, include_RESET=True):\n",
    "    for s in sequences:\n",
    "        if include_RESET:\n",
    "            s = ['RESET'] * order + s + ['RESET']\n",
    "        for i in range(len(s) - order):\n",
    "            yield (tuple(s[i:i+order]), s[i+order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mc(sequences, order):\n",
    "    counts = defaultdict(int)\n",
    "    row_names = {}\n",
    "    col_names = {}\n",
    "\n",
    "    for source,target in generate_transitions(sequences, order):\n",
    "        i,j = len(row_names),len(col_names)\n",
    "        if source not in row_names:\n",
    "            row_names[source] = i\n",
    "        if target not in col_names:\n",
    "            col_names[target] = j\n",
    "            \n",
    "        counts[(row_names[source],col_names[target])] += 1\n",
    "    \n",
    "#     print(row_names)\n",
    "#     print(col_names)\n",
    "#     print(counts)\n",
    "    \n",
    "    t_m = np.zeros(shape=(len(row_names),len(col_names)), dtype=np.float)\n",
    "    for i,j in counts.keys():\n",
    "        t_m[i,j] = counts[(i,j)]\n",
    "    t_m /= t_m.sum(axis=1)[:,np.newaxis]\n",
    "    \n",
    "    return t_m, list(row_names.keys()), list(col_names.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c) Comparing Markov Models (1.5)\n",
    "Implement three functions\n",
    "* `log_likelihood(sequences, t_m, row_names, col_names)`\n",
    "* `AIC(sequences, t_m, row_names, col_names)`\n",
    "* `BIC(sequences, t_m, row_names, col_names)`\n",
    "\n",
    "That compute the log likelihood, AIC and BIC scores for the provided sequences given the markov model specified through `t_m`, `row_names`, `col_names`. Use the natural logarithm.\n",
    "When calculating the log_likelihood skip transitions that have not been observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k(n_states, order):\n",
    "    k = (n_states ** order - n_states ** (order - 1) + 1) * (n_states - 1)\n",
    "    k -= 1\n",
    "    return k\n",
    "\n",
    "def log_likelihood(sequences, t_m, row_names, col_names):\n",
    "    order = len(next(iter(row_names)))\n",
    "    transitions = generate_transitions(sequences, order)\n",
    "    \n",
    "    ll = 0\n",
    "    for source,target in transitions:\n",
    "#         print(source,target)\n",
    "#         print(row_names)\n",
    "        i,j = row_names.index(source),col_names.index(target)\n",
    "        ll += np.log(t_m[i,j])\n",
    "    return ll\n",
    "    \n",
    "    \n",
    "def AIC(sequences, t_m, row_names, col_names):\n",
    "    order = len(next(iter(row_names)))\n",
    "    \n",
    "    n_rows, n_cols = len(row_names), len(col_names)\n",
    "    k = get_k(len(col_names), order)\n",
    "    \n",
    "    return 2 * k - 2 * log_likelihood(sequences, t_m, row_names, col_names)\n",
    "    \n",
    "    \n",
    "    \n",
    "def BIC(sequences, t_m, row_names, col_names):\n",
    "    order = len(next(iter(row_names)))\n",
    "    transitions = generate_transitions(sequences, order)\n",
    "    n = len(list(transitions))\n",
    "    \n",
    "    k = get_k(len(col_names), order)\n",
    "    \n",
    "    return np.log(n) * k - 2 * log_likelihood(sequences, t_m, row_names, col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d) Apply your model\n",
    "\n",
    "### d1) Load Sequences (1.5)\n",
    "Write a function `load_sequences(start, end)` that extracts the \"finished paths\" from the data published in https://snap.stanford.edu/data/wikispeedia.html, does preprocessing, cleans up all potential files and returns the transitions as a list of sequences. Disregard the rest of the information like ip_hash, time... The start and end variable specify a starting token and an ending token.\n",
    "\n",
    "\n",
    "Do all of this task in code (i.e. download (using the requests library), unzip, read and process the file). Make sure you **read the comments** at the beginning of the `paths_finished.tsv` file. Do **not** download the wikipedia articles along. After you are done, remove all potential files that you created during the process. There should be 1043 univariate sequences for `start, end = ('Asteroid', 'Viking')`.\n",
    "\n",
    "### d2) Eval Dataset (1)\n",
    "Fit a first order and second order model to the sequences from `'Asteroid'` to `'Viking'`. According to AIC and BIC, which model fits the data better?  Make a clear statement (as text), which model is to be preferred. Store that string in the variable `better_model`, also write that string to `better_model.txt`. Make sure you mention the AIC and BIC scores you obtained in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_wikipaths():\n",
    "    url = 'https://snap.stanford.edu/data/wikispeedia/wikispeedia_paths-and-graph.tar.gz'\n",
    "    with requests.get(url) as resp:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "            f.write(resp.content)\n",
    "            name = f.name\n",
    "        with tarfile.open(name) as tar:\n",
    "            tsv = tar.extractfile('wikispeedia_paths-and-graph/paths_finished.tsv')\n",
    "            df = pd.read_csv(tsv, sep='\\t', skiprows=16, header=None, \n",
    "                    names='hashedIpAddress timestamp durationInSec path rating'.split())\n",
    "            os.unlink(name)\n",
    "    return df\n",
    "\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_back_clicks(sequence):\n",
    "    stack = []\n",
    "    history = []\n",
    "    for c in sequence:\n",
    "        if c != '<':\n",
    "            stack.append(c)\n",
    "            history.append(c)\n",
    "        else:\n",
    "            stack.pop()\n",
    "            history.append(stack[-1])\n",
    "    return history\n",
    "\n",
    "def load_sequences(start, end):\n",
    "    global df\n",
    "    if df is None:\n",
    "        df = load_wikipaths()\n",
    "    \n",
    "    \n",
    "    return [process_back_clicks(s) for s in df.path.str.split(';').tolist() \n",
    "                            if s[0] == start and s[-1] == end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences1 = load_sequences('Asteroid', 'Viking')\n",
    "sequences2 = load_sequences('Asteroid', 'Viking')\n",
    "\n",
    "t_m1, row_names1, col_names1 = fit_mc(sequences1, 1)\n",
    "t_m2, row_names2, col_names2 = fit_mc(sequences2, 2)\n",
    "\n",
    "aic1 = AIC(sequences1, t_m1, row_names1, col_names1)\n",
    "aic2 = AIC(sequences2, t_m2, row_names2, col_names2)\n",
    "bic1 = BIC(sequences1, t_m1, row_names1, col_names1)\n",
    "bic2 = BIC(sequences2, t_m2, row_names2, col_names2)\n",
    "\n",
    "better_model = f'1-order model is better: AIC1={aic1:.2f} BIC1={bic1:.2f} vs AIC2={aic2:.2f} BIC2={bic2:.2f}'\n",
    "\n",
    "with open('better_model.txt', 'w') as f:\n",
    "    f.write(better_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Graphs (7 points total)\n",
    "\n",
    "In this task you are requested to do some basic tasks on graphs.\n",
    "\n",
    "## 2a) Building a graph (1)\n",
    "\n",
    "\n",
    "Represent the sequences for `start, end = (\"Apollo\", \"Skiing\")` from previous task d) as a directed graph in networkx. Use one of the standard networkx drawing functions. Exclude the RESET states. Show the plot here in the notebook and also save it as `apollo_skiing.png`. Include the direction and the node labels. Make sure the labels are at least somewhat readable. Argue whether this is a good visualization. Save this argument in a variable `apollo_skiing` as a string. Also write it to a file `apollo_skiing.txt`.\n",
    "\n",
    "\n",
    "## 2b) Classic link prediction metrics (0.5+0.5+1)\n",
    "\n",
    "In this task you should compute different metrics for link prediction. Do not use functions from networkx. Assume the graph is undirected.\n",
    "\n",
    "Write the functions `Jaccard_link(A)`, `Adamic_Adar_link(A)` and `Katz_link(A, beta)`.\n",
    "The parameter A is a binary adjancency matrix (assume a 2d-numpy array, it can be dense or sparse), beta is defined in the lecture. They both return a matrix with the corresponding scores for all the nodes in the network.\n",
    "Help for Katz: Search for geometric series of matrices ;). `raise` a ValueError if the provided `beta` is to large.\n",
    "\n",
    "The package networkx contains the Zacharyâ€™s Karate Club graph. It also contains functions that compute link scores. You can use it for testing.\n",
    "\n",
    "## 2c) Evaluate link prediction with mean inverse rank (1.5)\n",
    "\n",
    "To evaluate your link prediction we will use the mean inverse rank (mir). The basic idea is, that edges that really exist should be ranked higher by your metric in comparison with edges that do not exist.\n",
    "\n",
    "Starting from train/test split of the edges you rank the missing edges according to the metric in descending order. Now for all the heldout edges you obtain the place at which this ground truth edge is in the list of edges. This is the rank of the edge. You then calculate the mean of the inverse rank (1/rank) of all heldout edges.\n",
    "\n",
    "Write a function `cross_val_mir(G, func)` that computes the mean inverse rank for function `func` assume that the function takes a binary adjacency matrix and no other parameter. G is an undirected networkx graph. Use 10 fold cross validation on the edge set. Use `np.array_split` to split the edges in G into the 10 chunks. Return 1) the average mean inverse rank and 2) the standard deviation of the mir.\n",
    "\n",
    "Apply this function to the three link prediction metrics you previously defined. Use the Zachary Karate Club graph as provided in the networkx package. Store the results as a dict \n",
    "```python\n",
    "{\"Jaccard\" : 0.0,\n",
    " \"Adamic\" : 0.0,\n",
    " \"Katz\" : 0.0}\n",
    "```\n",
    "in `results_2c`. Store the beta you used for `Katz` in the variable `beta_2c`.\n",
    "\n",
    "## 2c) Random walks on a graph (1)\n",
    "\n",
    "Write a function `random_walks(A, start_node, length, rng)` that yields random walks of length `length` starting at node_index `start_node` on a graph given by a binary adjacency matrix `A`. Assume the graph is undirected. At each node the walker walks to another node at random. Do not use any restart conditions. At any node: Determine the k nodes to pick next, then choose the node which will be visited next with a call to `rng.randint(0,k)`.\n",
    "\n",
    "\n",
    "\n",
    "## 2d) Node2vec (1.5)\n",
    "\n",
    "Using your function `random_walks` along with the gensim.models.Word2Vec function to train a node2vec model.\n",
    "Write a helper function `get_corpus(A, length, n_per_node, rng)` that takes the binary adjacency matrix and creates a corpus (list of sequence of integers). It thereby uses the random walks function previously defined. To obtain `n_per_node` univariate sequences for each node. Go through the nodes in ascending order. Finally shuffle the list of sequences obtained using `np.permutation`. Return that sequence.\n",
    "\n",
    "Thereafter write a function `node2vec(G, dimensions, length, n_per_node, rng)` that trains Word2Vec embeddings of a specified number of dimensions for the networkx-graph G. It first creates a corpus for graph G, using the helper function. It then relabels the corpus such that the entries are the same as in the graph. It then uses gensim.models.word2vec to train a node embedding. Use default arguments for word2vec unless specified otherwise. (Only provide the corpus and the dimensions). Return the trained Word2Vec model.\n",
    "\n",
    "## 2e) Word2Vec/Node2Vec vs Glove (1)\n",
    "\n",
    "Explain the differences and similarities of the Anything2vec approach and Glove. Save your explanation as a string in the variable `glove`. Also write it to a file `glove.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
