{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webmining - Assignment 1\n",
    "\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. It familiarizes you with basics of *web crawling* and standard text preprocessing. It then takes a deep dive into *GloVe* one approach for obtaining word embeddings. To train GloVe we will frist construct the co-occurence matrix and then we will use adaptive stochastic gradient descent to minimize the cost function.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 2-3 people until 27.05.2020 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Double check if your code relies on presence of files or directories other than those mentioned\n",
    "    in given tasks. Tests run under Linux, hence don't use Windows style paths \n",
    "    (`some\\path`, `C:\\another\\path`). Also, use paths only that are relative to and within your\n",
    "    working directory (OK: `some/path`, `./some/path`; NOT OK: `/home/alice/python`, \n",
    "    `../../python`).\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for only the mean, prints an output instead of returning it, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members (you may add or remove items from the dictionary)\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Pavel',\n",
    "        'last_name': 'Raschetnov',\n",
    "        'student_id': 404839\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Anya',\n",
    "        'last_name': 'Poudyal',\n",
    "        'student_id': 391805\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Philipp',\n",
    "        'last_name': 'Stein',\n",
    "        'student_id': 397615\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Crawling web pages (total of 4 points)\n",
    "Consider the top 150 stackoverflow questions tagged with data-mining ordered by votes (e.g. the  questions from the 10 first pages accessible from here: https://stackoverflow.com/questions/tagged/data-mining?tab=votes&pagesize=15). Use the `BeautifulSoup`  and `requests` package.\n",
    "\n",
    "### a) Simple spidering (1.5)\n",
    "\n",
    "Write a function ```get_questions``` that takes a tag (like ```\"data-mining\"```) and a number `n` and returns a list containing the hyperlinks (as strings) to the top n questions as explained above. Assume the tag exists.\n",
    "\n",
    "(the first link for data-mining is: https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression)\n",
    "\n",
    "### b) Processing a page (1.5)\n",
    "\n",
    "Write a function ```process_question``` that takes a string hyperlink to a stackoverflow questions and returns a dictionary. It contains the text of the question, their comments and the answers to the question and comments. Keep in mind to remove: All images, tags, html tags, user_information, _code_ sections. Also remove information on edits, dates etc. Finally remove all functional text-content like share, edit, follow, flag, add comment (the kind of button things). Remove everything that is not inside the div with `id=\"mainbar\"`.\n",
    "\n",
    "The structure of the result is:\n",
    "\n",
    "```python\n",
    "{'title': 'The title',\n",
    " 'question': {'text' : 'How to learn web-mining?',\n",
    "              'comments':['Good question', 'Sounds\\n interesting']},\n",
    " 'answers' : [{'text':'Do a course at CSSH!', \n",
    "               'comments' : ['You will learn a lot', 'Good stuff']},\n",
    "              {'text':'Learn on youtube', 'comments' : []}, ]}\n",
    "```\n",
    "You can also find an example of the  processed page https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression on moodle.\n",
    "\n",
    "### c) Flatten the document (0.5)\n",
    "\n",
    "Write a function `flatten_question` that takes a dict of the structure from c) and returns a list of strings for that dict. Therefor merge the title with the flattened question. Then add the one string for each answer.\n",
    "The answer and question are flattened by first joining the comment strings with a `\" \"` and then joining the text and the comments in the same way. The text should preceed the comments.\n",
    "The returned list should look like:\n",
    "\n",
    "```\n",
    "['The title How to learn web-mining? Good question Sounds\\n interesting', 'Do a course at CSSH! You will learn a lot Good stuff', 'Learn on youtube ']\n",
    "```\n",
    "\n",
    "### d) Bringing it all together (0.5)\n",
    "\n",
    "Write a function `process_top_questions` that takes a tag and a number n and that processes the top n questions by votes as explained above. It returns a single list of strings (concatenated from the list of strings for each single answer). Thereby use the previously defined functions.\n",
    "\n",
    "Execute the function with the tag `\"data-mining\"` and n=150. Store the result in ```result_1```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(tag, n):\n",
    "    urls = []\n",
    "    page = 1\n",
    "    while len(urls) < n:\n",
    "        r = requests.get(f'https://stackoverflow.com/questions/tagged/{tag}?tab=votes&pagesize=50', \n",
    "                     params={'page': page})\n",
    "        soup = BeautifulSoup(r.text)\n",
    "\n",
    "        base_url = 'https://stackoverflow.com'\n",
    "        found = False\n",
    "        for a_tag in soup.select('div.question-summary h3 a'):\n",
    "            found = True\n",
    "            urls.append(base_url + a_tag['href'])\n",
    "            \n",
    "        if not found: \n",
    "            break\n",
    "            \n",
    "        page += 1\n",
    "    return urls[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(url):\n",
    "    print('processing ' + str(url))\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    mainbar = soup.select_one('div#mainbar')\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    result['title'] = soup.select_one('div#question-header h1 a').get_text()\n",
    "    \n",
    "    def parse_comments(el):\n",
    "        comments = []\n",
    "        for comment_el in el.select('div.comments div.comment-text'):\n",
    "            comment = ' '.join(x.get_text() for x in comment_el.select('span.comment-copy'))\n",
    "            comments.append(comment)\n",
    "        return comments\n",
    "    \n",
    "    \n",
    "    question_el = mainbar.select_one('div#question')\n",
    "    question_text = ' '.join(x.get_text() for x in question_el.select('div.post-text p'))\n",
    "    result['question'] = {\n",
    "        'text': question_text,\n",
    "        'comments': parse_comments(question_el)\n",
    "    }\n",
    "    \n",
    "    result['answers'] = []\n",
    "    for answer_el in mainbar.select('div#answers div.answer'):\n",
    "\n",
    "        text = '\\n'.join(p.get_text() for p in answer_el.select('div.post-text p'))\n",
    "\n",
    "        answer = {\n",
    "            'text': text,\n",
    "            'comments': parse_comments(answer_el)\n",
    "        }\n",
    "\n",
    "        result['answers'].append(answer)\n",
    "    \n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input\n",
    "d={'title': 'The title',\n",
    " 'question': {'text' : 'How to learn web-mining?',\n",
    "              'comments':['Good question', 'Sounds\\n interesting']},\n",
    " 'answers' : [{'text':'Do a course at CSSH!', \n",
    "               'comments' : ['You will learn a lot', 'Good stuff']},\n",
    "              {'text':'Learn on youtube', 'comments' : []}, ]}\n",
    "\n",
    "def flatten_question(d):\n",
    "    def merge_text_comments(d):\n",
    "        return d['text'] + ' ' + ' '.join(d['comments'])\n",
    "    \n",
    "    result =  [d['title'] + ' ' + merge_text_comments(d['question']),]\n",
    "    for a in d['answers']:\n",
    "        result.append(merge_text_comments(a))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_top_questions(tag, n):\n",
    "    from itertools import chain\n",
    "    \n",
    "    flattened_dicts = [flatten_question(process_question(q)) for q in get_questions(tag, n)]\n",
    "    return list(chain.from_iterable(flattened_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag,n = 'data-mining', 150\n",
    "result_1 = process_top_questions(tag, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing (total of 2 points)\n",
    "It is/was common to use stopword removal and stemming as a preparation for word_embeddings. You should use the stopword list provided by Python-nltk library.\n",
    "\n",
    "## a) The usual preprocessing (1)\n",
    "\n",
    "Write a function `to_tokens` that takes a string and performs tokenization, stopword removal and stemming. It returns a list of tokens. The tokens are in the same order they were in the initial string. Use the functions from the nltk library.\n",
    "To transform a string into tokens use the function `nltk.word_tokenize`.\n",
    "\n",
    "## b) Reduce vocabulary (1)\n",
    "Write a function `process_and_filter_corpus` that takes a list of strings and an integer as input. It applies the `to_tokens` function on each of those. The return values are\n",
    "\n",
    "1) the list of list of tokens. All tokens that appear less than min_support times (in the entire corpus) are __removed__\n",
    "\n",
    "2) a set of tokens that were removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pavel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pavel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(s):\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [t for t in tokens if not all(c in string.punctuation for c in t)]\n",
    "\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def process_and_filter_corpus(strings, min_support):\n",
    "    counts = Counter()\n",
    "    result = [to_tokens(s) for s in strings]\n",
    "    for tokens in result:\n",
    "        counts.update(tokens)\n",
    "    removed_tokens = {k for k,v in counts.items() if v < min_support}\n",
    "    \n",
    "    filtered_result = []\n",
    "    for tokens in result:\n",
    "        tokens = [t for t in tokens if t not in removed_tokens]\n",
    "        if len(tokens) > 0:\n",
    "            filtered_result.append(tokens)\n",
    "    \n",
    "    return filtered_result, removed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Glove (total of 10 points)\n",
    "### a) Computing the global word co-occurence matrix (3 = 2.5 + 0.5)\n",
    "We will now explore some steps of required for the GloVe word embedding.\n",
    "\n",
    "Write a function `get_cooc_matrix` that takes a list of list of tokens and an integer _context_size_ as input. It returns 1) a sparse co-occurrence matrix (a dict mapping pairs of integer indices to their cooc-score) and 2) a dictionary that maps a word to an index in the cooc matrix/dict. Do not use any non standard library for that. If two tokens are d places apart they get a score of 1/d. Only take into account words that are at most context_size apart from the central word.\n",
    "\n",
    "Example:\n",
    "\n",
    "For the corpus `[['bad', 'dog', 'bad', 'cat', 'thing'],['bad', 'dog']]` and context_size=2 the tokens 'dog' and 'thing' do not co-occur. While the tokens 'dog' and 'bad' have a cooc score of 3. The token pair 'bad' and 'bad' has a cooc score of 1.0.\n",
    "\n",
    "Additionally write a function `cooc_to_numpy` that takes the dict-sparse-representation and returns three numpy arrays. The first two are of type int and they contain the values for i and j respectively. The third array contains the cooc-score for that entry. They are sorted in a way that num it is first sorted ascending according to i and then ascending according to j. This sorting is only for reproducability, not necessary for the glove. We call the output of this function a coord_tpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooc_tpl (array([0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3]), array([0, 1, 2, 3, 0, 2, 0, 1, 3, 0, 2]), array([1. , 3. , 1. , 0.5, 3. , 0.5, 1. , 0.5, 1. , 0.5, 1. ]))\n",
      "vocab {'bad': 0, 'dog': 1, 'cat': 2, 'thing': 3}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_cooc_matrix(tokens_list, context_size):\n",
    "    vocab = {}\n",
    "    for tokens in tokens_list:\n",
    "        for t in tokens:\n",
    "            if t not in vocab:\n",
    "                vocab[t] = len(vocab)\n",
    "    \n",
    "    scores = defaultdict(float)\n",
    "    \n",
    "    for tokens in tokens_list:\n",
    "        for i in range(len(tokens)):\n",
    "            for d in range(1, context_size+1):\n",
    "                j1 = i - d\n",
    "                j2 = i + d\n",
    "                t = vocab[tokens[i]]\n",
    "                if j1 >= 0:\n",
    "                    t1 = vocab[tokens[j1]]\n",
    "                    scores[(t,t1)] += 1/d\n",
    "                if j2 < len(tokens):\n",
    "                    t2 = vocab[tokens[j2]]\n",
    "                    scores[(t,t2)] += 1/d\n",
    "    \n",
    "    return scores, vocab\n",
    "        \n",
    "def cooc_to_numpy(coocs):\n",
    "    i = []\n",
    "    j = []\n",
    "    s = []\n",
    "    for k in sorted(coocs.keys()):\n",
    "        i.append(k[0])\n",
    "        j.append(k[1])\n",
    "        s.append(coocs[k])\n",
    "    return (np.array(i), np.array(j), np.array(s))\n",
    "\n",
    "mini_corpus=[['bad', 'dog', 'bad', 'cat', 'thing'], ['bad', 'dog']]\n",
    "\n",
    "# results for mini_corpus:\n",
    "d,vocab = get_cooc_matrix(mini_corpus, 2)\n",
    "cooc_tpl = cooc_to_numpy(d)\n",
    "print('cooc_tpl',(\n",
    " np.array([0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3]),\n",
    " np.array([0, 1, 2, 3, 0, 2, 0, 1, 3, 0, 2]),\n",
    " np.array([1. , 3. , 1. , 0.5, 3. , 0.5, 1. , 0.5, 1. , 0.5, 1. ])))\n",
    "print('vocab', {'bad': 0, 'dog': 1, 'cat': 2, 'thing': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the global co-occurence matrix, it is time to use this information to train GloVe vectors.\n",
    "For this task we will use an (adaptive) stochastic gradient descent method.\n",
    "\n",
    "### b) Initialize Vectors and Gradients (1)\n",
    "Write a function `init_matrix`  that takes three arguments: 1) the size of the vocabulary $v$ 2) the size of the desired Glove vectors $n$ and 3) a random number generator which is initialized like: (https://numpy.org/doc/1.18/reference/random/index.html). It returns two matrices: 1) The matrix of glove vectors where each row is of size $n+1$ reflecting one glove vector + bias. It is initialized with random values such that they lie uniformly on the interval $[-l, l)$ with $l=\\frac{0.5}{n}$. Use one call to the generator only. 2) A matrix of similar shape initialized to all values being one. These are our corresponding squared gradients.\n",
    "\n",
    "### c) Compute loss (1.5)\n",
    "Write a function `compute_loss(v1, v2, b1, b2, cooc_score, max_score, alpha)` that computes the GloVe Loss for a single pair of vectors v1 and v2 their weights b1 and b2 with associated cooc_score and weighting parameters max_score and alpha. The function returns 1) the loss and 2) the part $g$ that is the part of the gradient, that is the same in both\n",
    "\n",
    "The GloVe Loss for a pair of indices i,j is: $loss(i,j) = f(C_{i,j}) \\cdot \\left(v_i \\cdot v_j + b_i + b_j -log(C_{i,j})\\right)^2$ with $C_{i,j}$ the cooc-score. The function function $f$ is defined as\n",
    "\n",
    "$f(x) = (\\frac{x}{max\\_score})^{\\alpha}$ if $x< max\\_score$\n",
    "\n",
    "$f(x) = 1 $ otherwise\n",
    "\n",
    "The shared part is: $g(i,j) = f(C_{i,j}) \\cdot \\left(v_i \\cdot v_j + b_i + b_j -log(C_{i,j})\\right)$\n",
    "\n",
    "### d) Computing the updates (0.5 each)\n",
    "\n",
    "Write a function `calc_gradient_vi(g, vj, eta, grad_clip)`  that takes the value $g$ and a vector $v_j$, the learning rate $\\eta$ and a gradient clipping value. It computes the gradient update for the $v_i$ vector without the bias. Applies gradient clipping such that the absolute value of each element of the gradient vector is at most grad_clip. Thereafter the gradient is multiplied with the learning rate and finally the validify function is applied to the gradient vector which is then returned.\n",
    "\n",
    "Write a similar function to compute the update for the bias b `calc_gradient_b(g, grad_clip)`. It computes the gradient update for the bias b. By clipping the gradient and therafter validifying the gradient. It then returns the gradient update.\n",
    "To compute the gradient update, drop the factor 2 that arises when differentiating the squared term. Also the gradient update for the bias terms does not include the learning rate.\n",
    "\n",
    "\n",
    "### e) Apply the update (1.5)\n",
    "\n",
    "Write a function `one_update(W, W_grad, pair, cooc_score, max_score, alpha, eta, grad_clip)`\n",
    "That performs an update of the vector matrix W and gradient matrix W_grad for a pair of word indices $(i,j)$=pair which with associated cooc_score. The remaining parameters should be clear from the previous functions.\n",
    "\n",
    "The matrix W is updated using the previously computed updates divided by the square root of the corresponding entries in the W_grad matrix. Keep in mind you have to walk into the opposite direction of the gradient. \n",
    "\n",
    "The W_grad matrix is updated using the sqared entry of the corresponding update (prior to dividing).\n",
    "\n",
    "The function returns the loss prior to upgrading. (The matrices W and W_grad are updated in place)\n",
    "\n",
    "### f) Write a training function (1)\n",
    "\n",
    "Write function `train(W, W_grad, cooc_tpl, n_epochs, rng, max_score, alpha, eta, grad_clip)` that trains W and W_grad matrix using the cooc_tpl for n_epochs. Before each epoch shuffle all arrays in the cocc_tpl in unison using one call to `rng.permutation`. \n",
    "\n",
    "### g) Bringing it all together (1)\n",
    "\n",
    "Write a function `train_corpus` that takes a corpus in the form of a list of strings.\n",
    "Preprocess each string as in task 2). Keep only words that have at least three occurences. \n",
    "\n",
    "Internally use: max_score=100, alpha=3/4, window size of 4, learning rate of 0.05, dimension of glove vectors = 50, grad_clip=100. Create a new `numpy.random.default_rng` instance, feed it a seed of 1. Train for 25 epochs.\n",
    "\n",
    "The function returns the trained W matrix.\n",
    "\n",
    "Apply it to your corpus that you obtained in task 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_matrix(v, n, rng):\n",
    "    glove_vectors = rng.uniform(-0.5/n, 0.5/n, size=(v,n+1))\n",
    "    squared_gradients = np.ones(shape=(v,n+1))\n",
    "    return glove_vectors, squared_gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(v1, v2, b1, b2, cooc_score, max_score, alpha):\n",
    "    def f(x):\n",
    "        if x < max_score:\n",
    "            return (x / max_score) ** alpha\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    m = (np.matmul(v1.T, v2) + b1 + b2 - np.log(cooc_score))\n",
    "    \n",
    "    loss = f(cooc_score) * m ** 2\n",
    "    g = f(cooc_score) * m\n",
    "    \n",
    "    return loss, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validify(grad):\n",
    "    if np.isnan(grad).sum()>0 or np.isinf(grad).sum()>0:\n",
    "        print('Warning: invalid value')\n",
    "        return np.zeros(np.shape(grad))\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "def calc_gradient_vi(fdiff, v, eta, grad_clip):\n",
    "    grad = fdiff * v\n",
    "    grad = np.clip(grad, -grad_clip, grad_clip)\n",
    "    grad *= eta\n",
    "    \n",
    "    validify(grad)\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def calc_gradient_b(fdiff, grad_clip):\n",
    "    grad = fdiff\n",
    "    grad = np.clip(grad, -grad_clip, grad_clip)\n",
    "    \n",
    "    validify(grad)\n",
    "    return grad\n",
    "\n",
    "def one_update(W, W_grad, pair, cooc_score, max_score, alpha, eta, grad_clip):\n",
    "    i,j = pair\n",
    "    vi, vj, bi, bj = W[i,:-1], W[j,:-1], W[i,-1], W[j,-1]\n",
    "    loss, fdiff = compute_loss(vi, vj, bi, bj, cooc_score, max_score, alpha)\n",
    "    grad_vi = calc_gradient_vi(fdiff, vj, eta, grad_clip)\n",
    "    grad_b = calc_gradient_b(fdiff, grad_clip)\n",
    "    \n",
    "    W[i,:-1] -= grad_vi / np.sqrt(W_grad[i,:-1])\n",
    "    W[i,-1] -= grad_b / np.sqrt(W_grad[i,-1])\n",
    "    \n",
    "    W_grad[i,:-1] += np.square(grad_vi)\n",
    "    W_grad[i,-1] += np.square(grad_b)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "# what is the right name for this function?? \n",
    "init_matrices = init_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W, W_grad, cooc_tpl, n_epochs, rng, max_score, alpha, eta, grad_clip):\n",
    "    # trying to permute with rng.permutation(x, axis=1) results in\n",
    "    # casting to float all 3 arrays\n",
    "    indices = range(len(cooc_tpl[0]))\n",
    "#     indices = rng.permutation(indices)\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'epoch #{epoch}')\n",
    "        indices = rng.permutation(indices)\n",
    "        for k in indices:\n",
    "            i,j = cooc_tpl[0][k], cooc_tpl[1][k]\n",
    "            s = cooc_tpl[2][k]\n",
    "            loss = one_update(W, W_grad, pair=(i,j), cooc_score=s, max_score=max_score,\n",
    "                      alpha=alpha, eta=eta, grad_clip=grad_clip)\n",
    "    print(f'loss on last pass: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Write a training function (1)\n",
    "\n",
    "Write function `train(W, W_grad, cooc_tpl, n_epochs, rng, max_score, alpha, eta, grad_clip)` that trains W and W_grad matrix using the cooc_tpl for n_epochs. Before each epoch shuffle all arrays in the cocc_tpl in unison using one call to `rng.permutation`. \n",
    "\n",
    "### g) Bringing it all together (1)\n",
    "\n",
    "Write a function `train_corpus` that takes a corpus in the form of a list of strings.\n",
    "Preprocess each string as in task 2). Keep only words that have at least three occurences. \n",
    "\n",
    "Internally use: max_score=100, alpha=3/4, window size of 4, learning rate of 0.05, dimension of glove vectors = 50, grad_clip=100. Create a new `numpy.random.default_rng` instance, feed it a seed of 1. Train for 25 epochs.\n",
    "\n",
    "The function returns the trained W matrix.\n",
    "\n",
    "Apply it to your corpus that you obtained in task 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_corpus(list_of_strings):\n",
    "    processed_strings = process_and_filter_corpus(list_of_strings, min_support=3)\n",
    "    \n",
    "    d,vocab = get_cooc_matrix(processed_strings, context_size=4)\n",
    "    cooc_tpl = cooc_to_numpy(d)\n",
    "    \n",
    "    rng = np.random.default_rng(seed=1)\n",
    "    W, W_grad = init_matrices(v=len(vocab), n=50, rng=rng)\n",
    "    \n",
    "    \n",
    "    train(W, W_grad, cooc_tpl, n_epochs=25, rng=rng, max_score=100, \n",
    "          alpha=3/4, eta=0.05, grad_clip=100)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_strings = process_and_filter_corpus(result_1, min_support=3)[0]\n",
    "\n",
    "# d,vocab = get_cooc_matrix(processed_strings, context_size=4)\n",
    "# cooc_tpl = cooc_to_numpy(d)\n",
    "\n",
    "# rng = np.random.default_rng(seed=1)\n",
    "# W, W_grad = init_matrices(v=len(vocab), n=50, rng=rng)\n",
    "\n",
    "# train(W, W_grad, cooc_tpl, n_epochs=25, rng=rng, max_score=100, \n",
    "#           alpha=3/4, eta=0.05, grad_clip=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00591081  0.22523185 -0.17792019]\n",
      " [ 0.22432472 -0.09408427 -0.03833678]\n",
      " [ 0.1638513  -0.04540043  0.02479684]\n",
      " [-0.23622044  0.12675655  0.01907166]]\n",
      "epoch #0\n",
      "epoch #1\n",
      "epoch #2\n",
      "epoch #3\n",
      "epoch #4\n",
      "epoch #5\n",
      "epoch #6\n",
      "epoch #7\n",
      "epoch #8\n",
      "epoch #9\n",
      "epoch #10\n",
      "epoch #11\n",
      "epoch #12\n",
      "epoch #13\n",
      "epoch #14\n",
      "epoch #15\n",
      "epoch #16\n",
      "epoch #17\n",
      "epoch #18\n",
      "epoch #19\n",
      "epoch #20\n",
      "epoch #21\n",
      "epoch #22\n",
      "epoch #23\n",
      "epoch #24\n",
      "loss on last pass: 0.33283633196107304\n",
      "[[ 0.3166805   0.07363883  0.13321995]\n",
      " [ 0.37128981  0.01910721  0.4799228 ]\n",
      " [-0.15818547  0.02831768 -0.22945749]\n",
      " [-0.34346481  0.03996652 -0.14912936]]\n",
      "[[ 1.00509379  1.00093216 10.39087987]\n",
      " [ 1.001393    1.00070234  7.96850913]\n",
      " [ 1.00333574  1.00022749  6.14571264]\n",
      " [ 1.00086463  1.00038586  4.82364196]]\n"
     ]
    }
   ],
   "source": [
    "# Small example of how it works:\n",
    "from numpy.random import default_rng\n",
    "\n",
    "# cooc_stuff\n",
    "cooc_dict, vocab = get_cooc_matrix(mini_corpus, 2)\n",
    "cooc_tpl = cooc_to_numpy(cooc_dict)\n",
    "\n",
    "rng = default_rng(1)\n",
    "W, W_grad = init_matrices(len(vocab), 2, rng)\n",
    "\n",
    "\n",
    "print(W)\n",
    "train(W, W_grad, cooc_tpl, n_epochs=25, rng=rng, max_score=2, alpha=0.8, eta=0.1, grad_clip=1)\n",
    "print(W)\n",
    "print(W_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What I obtain for code above\n",
    "\n",
    "# W init\n",
    "#[[ 0.00591081  0.22523185 -0.17792019]\n",
    "# [ 0.22432472 -0.09408427 -0.03833678]\n",
    "# [ 0.1638513  -0.04540043  0.02479684]\n",
    "# [-0.23622044  0.12675655  0.01907166]]\n",
    "\n",
    "\n",
    "# after training:\n",
    "#loss on last pass 0.7861722683546024\n",
    "#[[ 0.37861754  0.05811192  0.13090998]\n",
    "# [ 0.67430958 -0.00918572  0.58465184]\n",
    "# [-0.4713854   0.08926127 -0.31768868]\n",
    "# [-0.58445333  0.02673079 -0.0975816 ]]\n",
    "#[[ 1.01016694  1.0008865  12.1206226 ]\n",
    "# [ 1.003497    1.00143482  9.65947768]\n",
    "# [ 1.00864141  1.0002788   9.17531255]\n",
    "# [ 1.00211807  1.00053237  6.48303266]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
